/* tls_strp.c */
static int tls_strp_read_sock(struct tls_strparser *strp)
	int sz, inq;
	inq = tcp_inq(strp->sk);
	if (inq < 1)
	if (unlikely(strp->copy_mode))
	if (inq < strp->stm.full_len)
	if (!strp->stm.full_len) {
		tls_strp_load_anchor_with_queue(strp, inq);
		sz = tls_rx_msg_size(strp, strp->anchor);
		if (sz < 0) {
		strp->stm.full_len = sz;
		if (!strp->stm.full_len || inq < strp->stm.full_len)
	if (!tls_strp_check_queue_ok(strp))
	strp->msg_ready = 1;
void tls_strp_check_rcv(struct tls_strparser *strp)
	if (unlikely(strp->stopped) || strp->msg_ready)
	if (tls_strp_read_sock(strp) == -ENOMEM)
void tls_strp_data_ready(struct tls_strparser *strp)
	if (sock_owned_by_user_nocheck(strp->sk)) {
	tls_strp_check_rcv(strp);
static void tls_strp_work(struct work_struct *w)
	lock_sock(strp->sk);
	tls_strp_check_rcv(strp);
void tls_strp_msg_done(struct tls_strparser *strp)
	WARN_ON(!strp->stm.full_len);
	if (likely(!strp->copy_mode))
		tcp_read_done(strp->sk, strp->stm.full_len);
		tls_strp_flush_anchor_copy(strp);
	strp->msg_ready = 0;
	memset(&strp->stm, 0, sizeof(strp->stm));
	tls_strp_check_rcv(strp);
/* tls_sw.c */
static int
tls_rx_rec_wait(struct sock *sk, struct sk_psock *psock, bool nonblock,
	struct tls_context *tls_ctx = tls_get_ctx(sk);
	struct tls_sw_context_rx *ctx = tls_sw_ctx_rx(tls_ctx);
	DEFINE_WAIT_FUNC(wait, woken_wake_function);
	int ret = 0;
	long timeo;
	timeo = sock_rcvtimeo(sk, nonblock);
	while (!tls_strp_msg_ready(ctx)) {
		if (!sk_psock_queue_empty(psock))
		if (sk->sk_err)
		if (ret < 0)
		if (!skb_queue_empty(&sk->sk_receive_queue)) {
			tls_strp_check_rcv(&ctx->strp);
			if (tls_strp_msg_ready(ctx))
		if (sk->sk_shutdown & RCV_SHUTDOWN)
		if (sock_flag(sk, SOCK_DONE))
		if (!timeo)
		add_wait_queue(sk_sleep(sk), &wait);
		sk_set_bit(SOCKWQ_ASYNC_WAITDATA, sk);
		ret = sk_wait_event(sk, &timeo,
				    tls_strp_msg_ready(ctx) ||
				    !sk_psock_queue_empty(psock),
				    &wait);
		sk_clear_bit(SOCKWQ_ASYNC_WAITDATA, sk);
		remove_wait_queue(sk_sleep(sk), &wait);
		if (signal_pending(current))
static void tls_rx_rec_done(struct tls_sw_context_rx *ctx)
	tls_strp_msg_done(&ctx->strp);
int tls_sw_recvmsg(struct sock *sk,
		   struct msghdr *msg,
		   size_t len,
		   int flags,
	struct tls_context *tls_ctx = tls_get_ctx(sk);
	struct tls_sw_context_rx *ctx = tls_sw_ctx_rx(tls_ctx);
	struct tls_prot_info *prot = &tls_ctx->prot_info;
	ssize_t decrypted = 0, async_copy_bytes = 0;
	struct sk_psock *psock;
	unsigned char control = 0;
	size_t flushed_at = 0;
	struct strp_msg *rxm;
	struct tls_msg *tlm;
	ssize_t copied = 0;
	bool async = false;
	int target, err;
	bool is_kvec = iov_iter_is_kvec(&msg->msg_iter);
	bool is_peek = flags & MSG_PEEK;
	bool rx_more = false;
	bool released = true;
	bool bpf_strp_enabled;
	bool zc_capable;
	if (unlikely(flags & MSG_ERRQUEUE))
	err = tls_rx_reader_lock(sk, ctx, flags & MSG_DONTWAIT);
	if (err < 0)
	psock = sk_psock_get(sk);
	bpf_strp_enabled = sk_psock_strp_enabled(psock);
	if (err)
	err = process_rx_list(ctx, msg, &control, 0, len, is_peek, &rx_more);
	if (err < 0)
	copied = err;
	if (len <= copied || (copied && control != TLS_RECORD_TYPE_DATA) || rx_more)
	target = sock_rcvlowat(sk, flags & MSG_WAITALL, len);
	len = len - copied;
	zc_capable = !bpf_strp_enabled && !is_kvec && !is_peek &&
		ctx->zc_capable;
	decrypted = 0;
	while (len && (decrypted + copied < target || tls_strp_msg_ready(ctx))) {
		struct tls_decrypt_arg darg;
		int to_decrypt, chunk;
		err = tls_rx_rec_wait(sk, psock, flags & MSG_DONTWAIT,
				      released);
		if (err <= 0) {
			if (psock) {
				chunk = sk_msg_recvmsg(sk, psock, msg, len,
						       flags);
				if (chunk > 0) {
					decrypted += chunk;
					len -= chunk;
		memset(&darg.inargs, 0, sizeof(darg.inargs));
		rxm = strp_msg(tls_strp_msg(ctx));
		tlm = tls_msg(tls_strp_msg(ctx));
		to_decrypt = rxm->full_len - prot->overhead_size;
		if (zc_capable && to_decrypt <= len &&
		    tlm->control == TLS_RECORD_TYPE_DATA)
			darg.zc = true;
		if (tlm->control == TLS_RECORD_TYPE_DATA && !bpf_strp_enabled)
			darg.async = ctx->async_capable;
			darg.async = false;
		err = tls_rx_one_record(sk, msg, &darg);
		if (err < 0) {
		err = tls_record_content_type(msg, tls_msg(darg.skb), &control);
		if (err <= 0) {
			tls_rx_rec_done(ctx);
		released = tls_read_flush_backlog(sk, prot, len, to_decrypt,
						  decrypted + copied,
						  &flushed_at);
		rxm = strp_msg(darg.skb);
		chunk = rxm->full_len;
		tls_rx_rec_done(ctx);
		if (!darg.zc) {
			bool partially_consumed = chunk > len;
			struct sk_buff *skb = darg.skb;
			DEBUG_NET_WARN_ON_ONCE(darg.skb == ctx->strp.anchor);
			if (async) {
				async_copy_bytes += chunk;
				decrypted += chunk;
				len -= chunk;
				__skb_queue_tail(&ctx->rx_list, skb);
				if (unlikely(control != TLS_RECORD_TYPE_DATA))
			if (bpf_strp_enabled) {
				released = true;
				err = sk_psock_tls_strp_read(psock, skb);
				if (err != __SK_PASS) {
					rxm->offset = rxm->offset + rxm->full_len;
					rxm->full_len = 0;
			if (partially_consumed)
				chunk = len;
			err = skb_copy_datagram_msg(skb, rxm->offset,
						    msg, chunk);
			if (err < 0)
			if (is_peek) {
				peeked += chunk;
			if (partially_consumed) {
				rxm->offset += chunk;
				rxm->full_len -= chunk;
		decrypted += chunk;
		len -= chunk;
		msg->msg_flags |= MSG_EOR;
		if (control != TLS_RECORD_TYPE_DATA)
ssize_t tls_sw_splice_read(struct socket *sock,  loff_t *ppos,
			   size_t len, unsigned int flags)
	struct tls_context *tls_ctx = tls_get_ctx(sock->sk);
	struct tls_sw_context_rx *ctx = tls_sw_ctx_rx(tls_ctx);
	struct sock *sk = sock->sk;
	int err;
	err = tls_rx_reader_lock(sk, ctx, flags & SPLICE_F_NONBLOCK);
	if (err < 0)
	if (!skb_queue_empty(&ctx->rx_list)) {
		struct tls_decrypt_arg darg;
		err = tls_rx_rec_wait(sk, NULL, flags & SPLICE_F_NONBLOCK,
				      true);
		if (err <= 0)
		memset(&darg.inargs, 0, sizeof(darg.inargs));
		err = tls_rx_one_record(sk, NULL, &darg);
		if (err < 0) {
		tls_rx_rec_done(ctx);
int tls_sw_read_sock(struct sock *sk, read_descriptor_t *desc,
	struct tls_context *tls_ctx = tls_get_ctx(sk);
	struct tls_sw_context_rx *ctx = tls_sw_ctx_rx(tls_ctx);
	struct tls_prot_info *prot = &tls_ctx->prot_info;
	struct strp_msg *rxm = NULL;
	struct sk_buff *skb = NULL;
	struct sk_psock *psock;
	size_t flushed_at = 0;
	bool released = true;
	struct tls_msg *tlm;
	ssize_t decrypted;
	int err, used;
	psock = sk_psock_get(sk);
	if (psock) {
	err = tls_rx_reader_acquire(sk, ctx, true);
	if (err < 0)
	if (err)
	decrypted = 0;
		if (!skb_queue_empty(&ctx->rx_list)) {
			skb = __skb_dequeue(&ctx->rx_list);
			rxm = strp_msg(skb);
			tlm = tls_msg(skb);
			struct tls_decrypt_arg darg;
			err = tls_rx_rec_wait(sk, NULL, true, released);
			if (err <= 0)
			memset(&darg.inargs, 0, sizeof(darg.inargs));
			err = tls_rx_one_record(sk, NULL, &darg);
			if (err < 0) {
			released = tls_read_flush_backlog(sk, prot, INT_MAX,
							  0, decrypted,
							  &flushed_at);
			skb = darg.skb;
			rxm = strp_msg(skb);
			tlm = tls_msg(skb);
			decrypted += rxm->full_len;
			tls_rx_rec_done(ctx);
		if (tlm->control != TLS_RECORD_TYPE_DATA) {
		used = read_actor(desc, skb, rxm->offset, rxm->full_len);
		if (used <= 0) {
		copied += used;
		if (used < rxm->full_len) {
			rxm->offset += used;
			rxm->full_len -= used;
			if (!desc->count)
			if (!desc->count)
	} while (skb);
static void tls_data_ready(struct sock *sk)
	struct tls_context *tls_ctx = tls_get_ctx(sk);
	struct tls_sw_context_rx *ctx = tls_sw_ctx_rx(tls_ctx);
	tls_strp_data_ready(&ctx->strp);
	bool async;
	struct sk_buff *skb;
/* tls_device.c */
static int tls_push_data(struct sock *sk,
			 struct iov_iter *iter,
			 size_t size, int flags,
			 unsigned char record_type)
	struct tls_context *tls_ctx = tls_get_ctx(sk);
	struct tls_prot_info *prot = &tls_ctx->prot_info;
	struct tls_offload_context_tx *ctx = tls_offload_ctx_tx(tls_ctx);
	struct tls_record_info *record;
	int tls_push_record_flags;
	struct page_frag *pfrag;
	size_t orig_size = size;
	u32 max_open_record_len;
	bool more = false;
	bool done = false;
	int copy, rc = 0;
	long timeo;
	if (flags &
	    ~(MSG_MORE | MSG_DONTWAIT | MSG_NOSIGNAL |
	      MSG_SPLICE_PAGES | MSG_EOR))
	if ((flags & (MSG_MORE | MSG_EOR)) == (MSG_MORE | MSG_EOR))
	if (unlikely(sk->sk_err))
	flags |= MSG_SENDPAGE_DECRYPTED;
	tls_push_record_flags = flags | MSG_MORE;
	timeo = sock_sndtimeo(sk, flags & MSG_DONTWAIT);
	if (tls_is_partially_sent_record(tls_ctx)) {
		rc = tls_push_partial_record(sk, tls_ctx, flags);
		if (rc < 0)
	pfrag = sk_page_frag(sk);
	max_open_record_len = TLS_MAX_PAYLOAD_SIZE +
			      prot->prepend_size;
		rc = tls_do_allocation(sk, ctx, pfrag, prot->prepend_size);
		if (unlikely(rc)) {
			rc = sk_stream_wait_memory(sk, &timeo);
			if (!rc)
			record = ctx->open_record;
			if (!record)
			if (record_type != TLS_RECORD_TYPE_DATA) {
			} else if (record->len > prot->prepend_size) {
		record = ctx->open_record;
		copy = min_t(size_t, size, max_open_record_len - record->len);
		if (copy && (flags & MSG_SPLICE_PAGES)) {
			struct page_frag zc_pfrag;
			struct page **pages = &zc_pfrag.page;
			size_t off;
			rc = iov_iter_extract_pages(iter, &pages,
						    copy, 1, 0, &off);
			if (rc <= 0) {
			copy = rc;
			if (WARN_ON_ONCE(!sendpage_ok(zc_pfrag.page))) {
				iov_iter_revert(iter, copy);
			zc_pfrag.offset = off;
			zc_pfrag.size = copy;
			tls_append_frag(record, &zc_pfrag, copy);
		} else if (copy) {
			copy = min_t(size_t, copy, pfrag->size - pfrag->offset);
			rc = tls_device_copy_data(page_address(pfrag->page) +
						  pfrag->offset, copy,
						  iter);
			if (rc)
			tls_append_frag(record, pfrag, copy);
		size -= copy;
		if (!size) {
			tls_push_record_flags = flags;
			if (flags & MSG_MORE) {
			done = true;
		if (done || record->len >= max_open_record_len ||
		    (record->num_frags >= MAX_SKB_FRAGS - 1)) {
			tls_device_record_close(sk, tls_ctx, record,
						pfrag, record_type);
			rc = tls_push_record(sk,
					     tls_ctx,
					     ctx,
					     record,
					     tls_push_record_flags);
			if (rc < 0)
	} while (!done);
int tls_device_sendmsg(struct sock *sk, struct msghdr *msg, size_t size)
	unsigned char record_type = TLS_RECORD_TYPE_DATA;
	struct tls_context *tls_ctx = tls_get_ctx(sk);
	int rc;
	if (!tls_ctx->zerocopy_sendfile)
		msg->msg_flags &= ~MSG_SPLICE_PAGES;
	lock_sock(sk);
	if (unlikely(msg->msg_controllen)) {
		rc = tls_process_cmsg(sk, msg, &record_type);
		if (rc)
	rc = tls_push_data(sk, &msg->msg_iter, size, msg->msg_flags,
			   record_type);
void tls_device_splice_eof(struct socket *sock)
	struct sock *sk = sock->sk;
	struct tls_context *tls_ctx = tls_get_ctx(sk);
	struct iov_iter iter = {};
	if (!tls_is_partially_sent_record(tls_ctx))
	mutex_lock(&tls_ctx->tx_lock);
	lock_sock(sk);
	if (tls_is_partially_sent_record(tls_ctx)) {
		iov_iter_bvec(&iter, ITER_SOURCE, NULL, 0, 0);
		tls_push_data(sk, &iter, 0, 0, TLS_RECORD_TYPE_DATA);
static int tls_device_push_pending_record(struct sock *sk, int flags)
	struct iov_iter iter;
	iov_iter_kvec(&iter, ITER_SOURCE, NULL, 0, 0);
	return tls_push_data(sk, &iter, 0, flags, TLS_RECORD_TYPE_DATA);
/* tls_main.c */
static int do_tls_setsockopt_conf(struct sock *sk, sockptr_t optval,
				  unsigned int optlen, int tx)
	struct tls_crypto_info *crypto_info;
	struct tls_context *ctx = tls_get_ctx(sk);
	const struct tls_cipher_desc *cipher_desc;
	int rc = 0;
	int conf;
	if (sockptr_is_null(optval) || (optlen < sizeof(*crypto_info)))
	if (tx) {
		crypto_info = &ctx->crypto_send.info;
		crypto_info = &ctx->crypto_recv.info;
	if (TLS_CRYPTO_INFO_READY(crypto_info))
	rc = copy_from_sockptr(crypto_info, optval, sizeof(*crypto_info));
	if (rc) {
	rc = validate_crypto_info(crypto_info, alt_crypto_info);
	if (rc)
	cipher_desc = get_cipher_desc(crypto_info->cipher_type);
	if (!cipher_desc) {
	if (optlen != cipher_desc->crypto_info) {
	if (rc) {
	if (tx) {
		rc = tls_set_device_offload(sk);
		conf = TLS_HW;
		if (!rc) {
			if (rc)
			conf = TLS_SW;
		rc = tls_set_device_offload_rx(sk, ctx);
		conf = TLS_HW;
		if (!rc) {
			if (rc)
			conf = TLS_SW;
		tls_sw_strparser_arm(sk, ctx);
	if (tx)
		ctx->tx_conf = conf;
		ctx->rx_conf = conf;
	update_sk_prot(sk, ctx);
	if (tx) {
		struct tls_sw_context_rx *rx_ctx = tls_sw_ctx_rx(ctx);
		tls_strp_check_rcv(&rx_ctx->strp);
static int do_tls_setsockopt(struct sock *sk, int optname, sockptr_t optval,
			     unsigned int optlen)
	switch (optname) {
		lock_sock(sk);
		rc = do_tls_setsockopt_conf(sk, optval, optlen,
					    optname == TLS_TX);
static int tls_setsockopt(struct sock *sk, int level, int optname,
			  sockptr_t optval, unsigned int optlen)
	struct tls_context *ctx = tls_get_ctx(sk);
	if (level != SOL_TLS)
	return do_tls_setsockopt(sk, optname, optval, optlen);
