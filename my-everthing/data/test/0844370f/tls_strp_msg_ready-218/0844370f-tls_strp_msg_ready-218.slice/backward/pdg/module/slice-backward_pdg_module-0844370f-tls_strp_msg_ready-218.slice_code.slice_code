/* tls.h */
static inline bool tls_strp_msg_ready(struct tls_sw_context_rx *ctx)
	return ctx->strp.msg_ready;
/* tls_sw.c */
static int
tls_rx_rec_wait(struct sock *sk, struct sk_psock *psock, bool nonblock,
	struct tls_context *tls_ctx = tls_get_ctx(sk);
	struct tls_sw_context_rx *ctx = tls_sw_ctx_rx(tls_ctx);
	DEFINE_WAIT_FUNC(wait, woken_wake_function);
	int ret = 0;
	long timeo;
	timeo = sock_rcvtimeo(sk, nonblock);
	while (!tls_strp_msg_ready(ctx)) {
		if (!sk_psock_queue_empty(psock))
		if (sk->sk_err)
		if (ret < 0)
		if (!skb_queue_empty(&sk->sk_receive_queue)) {
			tls_strp_check_rcv(&ctx->strp);
			if (tls_strp_msg_ready(ctx))
		if (sk->sk_shutdown & RCV_SHUTDOWN)
		if (sock_flag(sk, SOCK_DONE))
		if (!timeo)
		add_wait_queue(sk_sleep(sk), &wait);
		sk_set_bit(SOCKWQ_ASYNC_WAITDATA, sk);
		ret = sk_wait_event(sk, &timeo,
				    tls_strp_msg_ready(ctx) ||
				    !sk_psock_queue_empty(psock),
				    &wait);
		sk_clear_bit(SOCKWQ_ASYNC_WAITDATA, sk);
		remove_wait_queue(sk_sleep(sk), &wait);
		if (signal_pending(current))
int tls_sw_recvmsg(struct sock *sk,
		   struct msghdr *msg,
		   size_t len,
		   int flags,
	struct tls_context *tls_ctx = tls_get_ctx(sk);
	struct tls_sw_context_rx *ctx = tls_sw_ctx_rx(tls_ctx);
	struct tls_prot_info *prot = &tls_ctx->prot_info;
	ssize_t decrypted = 0, async_copy_bytes = 0;
	struct sk_psock *psock;
	unsigned char control = 0;
	size_t flushed_at = 0;
	struct strp_msg *rxm;
	struct tls_msg *tlm;
	ssize_t copied = 0;
	bool async = false;
	int target, err;
	bool is_kvec = iov_iter_is_kvec(&msg->msg_iter);
	bool is_peek = flags & MSG_PEEK;
	bool rx_more = false;
	bool released = true;
	bool bpf_strp_enabled;
	bool zc_capable;
	if (unlikely(flags & MSG_ERRQUEUE))
	err = tls_rx_reader_lock(sk, ctx, flags & MSG_DONTWAIT);
	if (err < 0)
	psock = sk_psock_get(sk);
	bpf_strp_enabled = sk_psock_strp_enabled(psock);
	if (err)
	err = process_rx_list(ctx, msg, &control, 0, len, is_peek, &rx_more);
	if (err < 0)
	copied = err;
	if (len <= copied || (copied && control != TLS_RECORD_TYPE_DATA) || rx_more)
	target = sock_rcvlowat(sk, flags & MSG_WAITALL, len);
	len = len - copied;
	zc_capable = !bpf_strp_enabled && !is_kvec && !is_peek &&
		ctx->zc_capable;
	decrypted = 0;
	while (len && (decrypted + copied < target || tls_strp_msg_ready(ctx))) {
		struct tls_decrypt_arg darg;
		int to_decrypt, chunk;
		err = tls_rx_rec_wait(sk, psock, flags & MSG_DONTWAIT,
				      released);
		if (err <= 0) {
			if (psock) {
				chunk = sk_msg_recvmsg(sk, psock, msg, len,
						       flags);
				if (chunk > 0) {
					decrypted += chunk;
					len -= chunk;
		memset(&darg.inargs, 0, sizeof(darg.inargs));
		rxm = strp_msg(tls_strp_msg(ctx));
		tlm = tls_msg(tls_strp_msg(ctx));
		to_decrypt = rxm->full_len - prot->overhead_size;
		if (zc_capable && to_decrypt <= len &&
		    tlm->control == TLS_RECORD_TYPE_DATA)
			darg.zc = true;
		if (tlm->control == TLS_RECORD_TYPE_DATA && !bpf_strp_enabled)
			darg.async = ctx->async_capable;
			darg.async = false;
		err = tls_rx_one_record(sk, msg, &darg);
		if (err < 0) {
		err = tls_record_content_type(msg, tls_msg(darg.skb), &control);
		if (err <= 0) {
		released = tls_read_flush_backlog(sk, prot, len, to_decrypt,
						  decrypted + copied,
						  &flushed_at);
		rxm = strp_msg(darg.skb);
		chunk = rxm->full_len;
		tls_rx_rec_done(ctx);
		if (!darg.zc) {
			bool partially_consumed = chunk > len;
			struct sk_buff *skb = darg.skb;
			DEBUG_NET_WARN_ON_ONCE(darg.skb == ctx->strp.anchor);
			if (async) {
				async_copy_bytes += chunk;
				decrypted += chunk;
				len -= chunk;
				__skb_queue_tail(&ctx->rx_list, skb);
				if (unlikely(control != TLS_RECORD_TYPE_DATA))
			if (bpf_strp_enabled) {
				released = true;
				err = sk_psock_tls_strp_read(psock, skb);
				if (err != __SK_PASS) {
					rxm->offset = rxm->offset + rxm->full_len;
					rxm->full_len = 0;
			if (partially_consumed)
				chunk = len;
			err = skb_copy_datagram_msg(skb, rxm->offset,
						    msg, chunk);
			if (err < 0)
			if (is_peek) {
				peeked += chunk;
			if (partially_consumed) {
				rxm->offset += chunk;
				rxm->full_len -= chunk;
		decrypted += chunk;
		len -= chunk;
		msg->msg_flags |= MSG_EOR;
		if (control != TLS_RECORD_TYPE_DATA)
ssize_t tls_sw_splice_read(struct socket *sock,  loff_t *ppos,
			   size_t len, unsigned int flags)
	struct tls_context *tls_ctx = tls_get_ctx(sock->sk);
	struct tls_sw_context_rx *ctx = tls_sw_ctx_rx(tls_ctx);
	struct sock *sk = sock->sk;
	int err;
	err = tls_rx_reader_lock(sk, ctx, flags & SPLICE_F_NONBLOCK);
	if (err < 0)
	if (!skb_queue_empty(&ctx->rx_list)) {
		err = tls_rx_rec_wait(sk, NULL, flags & SPLICE_F_NONBLOCK,
				      true);
int tls_sw_read_sock(struct sock *sk, read_descriptor_t *desc,
	struct tls_context *tls_ctx = tls_get_ctx(sk);
	struct tls_sw_context_rx *ctx = tls_sw_ctx_rx(tls_ctx);
	struct tls_prot_info *prot = &tls_ctx->prot_info;
	struct strp_msg *rxm = NULL;
	struct sk_buff *skb = NULL;
	struct sk_psock *psock;
	size_t flushed_at = 0;
	bool released = true;
	struct tls_msg *tlm;
	ssize_t decrypted;
	int err, used;
	psock = sk_psock_get(sk);
	if (psock) {
	err = tls_rx_reader_acquire(sk, ctx, true);
	if (err < 0)
	if (err)
	decrypted = 0;
		if (!skb_queue_empty(&ctx->rx_list)) {
			skb = __skb_dequeue(&ctx->rx_list);
			rxm = strp_msg(skb);
			tlm = tls_msg(skb);
			struct tls_decrypt_arg darg;
			err = tls_rx_rec_wait(sk, NULL, true, released);
			if (err <= 0)
			memset(&darg.inargs, 0, sizeof(darg.inargs));
			err = tls_rx_one_record(sk, NULL, &darg);
			if (err < 0) {
			released = tls_read_flush_backlog(sk, prot, INT_MAX,
							  0, decrypted,
							  &flushed_at);
			skb = darg.skb;
			rxm = strp_msg(skb);
			tlm = tls_msg(skb);
			decrypted += rxm->full_len;
			tls_rx_rec_done(ctx);
		if (tlm->control != TLS_RECORD_TYPE_DATA) {
		used = read_actor(desc, skb, rxm->offset, rxm->full_len);
		if (used <= 0) {
		copied += used;
		if (used < rxm->full_len) {
			rxm->offset += used;
			rxm->full_len -= used;
			if (!desc->count)
			if (!desc->count)
	} while (skb);
bool tls_sw_sock_is_readable(struct sock *sk)
	struct tls_context *tls_ctx = tls_get_ctx(sk);
	struct tls_sw_context_rx *ctx = tls_sw_ctx_rx(tls_ctx);
	bool ingress_empty = true;
	struct sk_psock *psock;
	psock = sk_psock(sk);
	if (psock)
		ingress_empty = list_empty(&psock->ingress_msg);
	return !ingress_empty || tls_strp_msg_ready(ctx) ||
	bool async;
	struct sk_buff *skb;
/* tls_main.c */
static __poll_t tls_sk_poll(struct file *file, struct socket *sock,
	struct tls_sw_context_rx *ctx;
	struct tls_context *tls_ctx;
	struct sock *sk = sock->sk;
	u8 shutdown;
	int state;
	state = inet_sk_state_load(sk);
	shutdown = READ_ONCE(sk->sk_shutdown);
	if (unlikely(state != TCP_ESTABLISHED || shutdown & RCV_SHUTDOWN))
	tls_ctx = tls_get_ctx(sk);
	ctx = tls_sw_ctx_rx(tls_ctx);
	if (skb_queue_empty_lockless(&ctx->rx_list) &&
	    !tls_strp_msg_ready(ctx) &&
