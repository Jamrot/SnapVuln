file: /* tls_strp.c */
574: void tls_strp_msg_done(struct tls_strparser *strp)
576: 	WARN_ON(!strp->stm.full_len);
578: 	if (likely(!strp->copy_mode))
579: 		tcp_read_done(strp->sk, strp->stm.full_len);
581: 		tls_strp_flush_anchor_copy(strp);
583: 	strp->msg_ready = 0;
584: 	memset(&strp->stm, 0, sizeof(strp->stm));
586: 	tls_strp_check_rcv(strp);
file: /* tls_sw.c */
1005: static int tls_sw_sendmsg_locked(struct sock *sk, struct msghdr *msg,
1008: 	long timeo = sock_sndtimeo(sk, msg->msg_flags & MSG_DONTWAIT);
1009: 	struct tls_context *tls_ctx = tls_get_ctx(sk);
1010: 	struct tls_prot_info *prot = &tls_ctx->prot_info;
1011: 	struct tls_sw_context_tx *ctx = tls_sw_ctx_tx(tls_ctx);
1012: 	bool async_capable = ctx->async_capable;
1013: 	unsigned char record_type = TLS_RECORD_TYPE_DATA;
1014: 	bool is_kvec = iov_iter_is_kvec(&msg->msg_iter);
1015: 	bool eor = !(msg->msg_flags & MSG_MORE);
1017: 	ssize_t copied = 0;
1021: 	int num_async = 0;
1026: 	int ret = 0;
1028: 	if (!eor && (msg->msg_flags & MSG_EOR))
1031: 	if (unlikely(msg->msg_controllen)) {
1032: 		ret = tls_process_cmsg(sk, msg, &record_type);
1033: 		if (ret) {
1034: 			if (ret == -EINPROGRESS)
1035: 				num_async++;
1036: 			else if (ret != -EAGAIN)
1041: 	while (msg_data_left(msg)) {
1042: 		if (sk->sk_err) {
1043: 			ret = -sk->sk_err;
1047: 		if (ctx->open_rec)
1048: 			rec = ctx->open_rec;
1050: 			rec = ctx->open_rec = tls_get_rec(sk);
1051: 		if (!rec) {
1052: 			ret = -ENOMEM;
1056: 		msg_pl = &rec->msg_plaintext;
1057: 		msg_en = &rec->msg_encrypted;
1059: 		orig_size = msg_pl->sg.size;
1060: 		full_record = false;
1061: 		try_to_copy = msg_data_left(msg);
1062: 		record_room = TLS_MAX_PAYLOAD_SIZE - msg_pl->sg.size;
1063: 		if (try_to_copy >= record_room) {
1064: 			try_to_copy = record_room;
1065: 			full_record = true;
1068: 		required_size = msg_pl->sg.size + try_to_copy +
1069: 				prot->overhead_size;
1071: 		if (!sk_stream_memory_free(sk))
1075: 		ret = tls_alloc_encrypted_msg(sk, required_size);
1076: 		if (ret) {
1077: 			if (ret != -ENOSPC)
1084: 			try_to_copy -= required_size - msg_en->sg.size;
1085: 			full_record = true;
1088: 		if (try_to_copy && (msg->msg_flags & MSG_SPLICE_PAGES)) {
1089: 			ret = tls_sw_sendmsg_splice(sk, msg, msg_pl,
1090: 						    try_to_copy, &copied);
1091: 			if (ret < 0)
1093: 			tls_ctx->pending_open_record_frags = true;
1095: 			if (sk_msg_full(msg_pl))
1096: 				full_record = true;
1098: 			if (full_record || eor)
1103: 		if (!is_kvec && (full_record || eor) && !async_capable) {
1104: 			u32 first = msg_pl->sg.end;
1106: 			ret = sk_msg_zerocopy_from_iter(sk, &msg->msg_iter,
1107: 							msg_pl, try_to_copy);
1108: 			if (ret)
1112: 			copied += try_to_copy;
1114: 			sk_msg_sg_copy_set(msg_pl, first);
1115: 			ret = bpf_exec_tx_verdict(msg_pl, sk, full_record,
1116: 						  record_type, &copied,
1117: 						  msg->msg_flags);
1118: 			if (ret) {
1119: 				if (ret == -EINPROGRESS)
1120: 					num_async++;
1121: 				else if (ret == -ENOMEM)
1123: 				else if (ctx->open_rec && ret == -ENOSPC)
1125: 				else if (ret != -EAGAIN)
1130: 			copied -= try_to_copy;
1131: 			sk_msg_sg_copy_clear(msg_pl, first);
1132: 			iov_iter_revert(&msg->msg_iter,
1133: 					msg_pl->sg.size - orig_size);
1135: 			sk_msg_trim(sk, msg_pl, orig_size);
1138: 		required_size = msg_pl->sg.size + try_to_copy;
1140: 		ret = tls_clone_plaintext_msg(sk, required_size);
1141: 		if (ret) {
1142: 			if (ret != -ENOSPC)
1149: 			try_to_copy -= required_size - msg_pl->sg.size;
1150: 			full_record = true;
1151: 			sk_msg_trim(sk, msg_en,
1152: 				    msg_pl->sg.size + prot->overhead_size);
1155: 		if (try_to_copy) {
1156: 			ret = sk_msg_memcopy_from_iter(sk, &msg->msg_iter,
1157: 						       msg_pl, try_to_copy);
1158: 			if (ret < 0)
1165: 		tls_ctx->pending_open_record_frags = true;
1166: 		copied += try_to_copy;
1168: 		if (full_record || eor) {
1169: 			ret = bpf_exec_tx_verdict(msg_pl, sk, full_record,
1170: 						  record_type, &copied,
1171: 						  msg->msg_flags);
1172: 			if (ret) {
1173: 				if (ret == -EINPROGRESS)
1174: 					num_async++;
1175: 				else if (ret == -ENOMEM)
1177: 				else if (ret != -EAGAIN) {
1178: 					if (ret == -ENOSPC)
1179: 						ret = 0;
1188: 		set_bit(SOCK_NOSPACE, &sk->sk_socket->flags);
1190: 		ret = sk_stream_wait_memory(sk, &timeo);
1191: 		if (ret) {
1193: 			if (ctx->open_rec)
1194: 				tls_trim_both_msgs(sk, orig_size);
1198: 		if (ctx->open_rec && msg_en->sg.size < required_size)
1202: 	if (!num_async) {
1204: 	} else if (num_zc) {
1208: 		err = tls_encrypt_async_wait(ctx);
1209: 		if (err) {
1210: 			ret = err;
1216: 	if (test_and_clear_bit(BIT_TX_SCHEDULED, &ctx->tx_bitmask)) {
1218: 		tls_tx_records(sk, msg->msg_flags);
1222: 	ret = sk_stream_error(sk, msg->msg_flags, ret);
1226: int tls_sw_sendmsg(struct sock *sk, struct msghdr *msg, size_t size)
1228: 	struct tls_context *tls_ctx = tls_get_ctx(sk);
1231: 	if (msg->msg_flags & ~(MSG_MORE | MSG_DONTWAIT | MSG_NOSIGNAL |
1232: 			       MSG_CMSG_COMPAT | MSG_SPLICE_PAGES | MSG_EOR |
1233: 			       MSG_SENDPAGE_NOPOLICY))
1237: 	if (ret)
1239: 	lock_sock(sk);
1240: 	ret = tls_sw_sendmsg_locked(sk, msg, size);
1241: 	release_sock(sk);
1249: void tls_sw_splice_eof(struct socket *sock)
1251: 	struct sock *sk = sock->sk;
1252: 	struct tls_context *tls_ctx = tls_get_ctx(sk);
1253: 	struct tls_sw_context_tx *ctx = tls_sw_ctx_tx(tls_ctx);
1256: 	ssize_t copied = 0;
1257: 	bool retrying = false;
1260: 	if (!ctx->open_rec)
1264: 	lock_sock(sk);
1268: 	rec = ctx->open_rec;
1269: 	if (!rec)
1272: 	msg_pl = &rec->msg_plaintext;
1273: 	if (msg_pl->sg.size == 0)
1277: 	ret = bpf_exec_tx_verdict(msg_pl, sk, false, TLS_RECORD_TYPE_DATA,
1278: 				  &copied, 0);
1279: 	switch (ret) {
1282: 		if (retrying)
1775: static void tls_rx_rec_done(struct tls_sw_context_rx *ctx)
1777: 	tls_strp_msg_done(&ctx->strp);
1950: int tls_sw_recvmsg(struct sock *sk,
1951: 		   struct msghdr *msg,
1952: 		   size_t len,
1953: 		   int flags,
1956: 	struct tls_context *tls_ctx = tls_get_ctx(sk);
1957: 	struct tls_sw_context_rx *ctx = tls_sw_ctx_rx(tls_ctx);
1958: 	struct tls_prot_info *prot = &tls_ctx->prot_info;
1961: 	unsigned char control = 0;
1962: 	size_t flushed_at = 0;
1967: 	bool async = false;
1969: 	bool is_kvec = iov_iter_is_kvec(&msg->msg_iter);
1970: 	bool is_peek = flags & MSG_PEEK;
1971: 	bool rx_more = false;
1972: 	bool released = true;
1976: 	if (unlikely(flags & MSG_ERRQUEUE))
1977: 		return sock_recv_errqueue(sk, msg, len, SOL_IP, IP_RECVERR);
1979: 	err = tls_rx_reader_lock(sk, ctx, flags & MSG_DONTWAIT);
1980: 	if (err < 0)
1982: 	psock = sk_psock_get(sk);
1983: 	bpf_strp_enabled = sk_psock_strp_enabled(psock);
1987: 	if (err)
1991: 	err = process_rx_list(ctx, msg, &control, 0, len, is_peek, &rx_more);
1992: 	if (err < 0)
1995: 	copied = err;
1996: 	if (len <= copied || (copied && control != TLS_RECORD_TYPE_DATA) || rx_more)
1999: 	target = sock_rcvlowat(sk, flags & MSG_WAITALL, len);
2000: 	len = len - copied;
2002: 	zc_capable = !bpf_strp_enabled && !is_kvec && !is_peek &&
2003: 		ctx->zc_capable;
2004: 	decrypted = 0;
2005: 	while (len && (decrypted + copied < target || tls_strp_msg_ready(ctx))) {
2009: 		err = tls_rx_rec_wait(sk, psock, flags & MSG_DONTWAIT,
2010: 				      released);
2011: 		if (err <= 0) {
2012: 			if (psock) {
2013: 				chunk = sk_msg_recvmsg(sk, psock, msg, len,
2014: 						       flags);
2015: 				if (chunk > 0) {
2016: 					decrypted += chunk;
2017: 					len -= chunk;
2024: 		memset(&darg.inargs, 0, sizeof(darg.inargs));
2026: 		rxm = strp_msg(tls_strp_msg(ctx));
2027: 		tlm = tls_msg(tls_strp_msg(ctx));
2029: 		to_decrypt = rxm->full_len - prot->overhead_size;
2031: 		if (zc_capable && to_decrypt <= len &&
2032: 		    tlm->control == TLS_RECORD_TYPE_DATA)
2033: 			darg.zc = true;
2036: 		if (tlm->control == TLS_RECORD_TYPE_DATA && !bpf_strp_enabled)
2037: 			darg.async = ctx->async_capable;
2039: 			darg.async = false;
2041: 		err = tls_rx_one_record(sk, msg, &darg);
2042: 		if (err < 0) {
2043: 			tls_err_abort(sk, -EBADMSG);
2056: 		err = tls_record_content_type(msg, tls_msg(darg.skb), &control);
2057: 		if (err <= 0) {
2059: 			tls_rx_rec_done(ctx);
2066: 		released = tls_read_flush_backlog(sk, prot, len, to_decrypt,
2067: 						  decrypted + copied,
2068: 						  &flushed_at);
2071: 		rxm = strp_msg(darg.skb);
2072: 		chunk = rxm->full_len;
2073: 		tls_rx_rec_done(ctx);
2075: 		if (!darg.zc) {
2076: 			bool partially_consumed = chunk > len;
2077: 			struct sk_buff *skb = darg.skb;
2079: 			DEBUG_NET_WARN_ON_ONCE(darg.skb == ctx->strp.anchor);
2081: 			if (async) {
2084: 				async_copy_bytes += chunk;
2086: 				decrypted += chunk;
2087: 				len -= chunk;
2088: 				__skb_queue_tail(&ctx->rx_list, skb);
2089: 				if (unlikely(control != TLS_RECORD_TYPE_DATA))
2094: 			if (bpf_strp_enabled) {
2095: 				released = true;
2096: 				err = sk_psock_tls_strp_read(psock, skb);
2097: 				if (err != __SK_PASS) {
2098: 					rxm->offset = rxm->offset + rxm->full_len;
2099: 					rxm->full_len = 0;
2106: 			if (partially_consumed)
2107: 				chunk = len;
2109: 			err = skb_copy_datagram_msg(skb, rxm->offset,
2110: 						    msg, chunk);
2111: 			if (err < 0)
2114: 			if (is_peek) {
2115: 				peeked += chunk;
2119: 			if (partially_consumed) {
2120: 				rxm->offset += chunk;
2121: 				rxm->full_len -= chunk;
2128: 		decrypted += chunk;
2129: 		len -= chunk;
2134: 		msg->msg_flags |= MSG_EOR;
2135: 		if (control != TLS_RECORD_TYPE_DATA)
2140: 	if (async) {
2147: 		if (ret) {
2155: 		if (is_peek)
2156: 			err = process_rx_list(ctx, msg, &control, copied + peeked,
2159: 			err = process_rx_list(ctx, msg, &control, 0,
2169: 	tls_rx_reader_unlock(sk, ctx);
2170: 	if (psock)
2171: 		sk_psock_put(sk, psock);
2175: ssize_t tls_sw_splice_read(struct socket *sock,  loff_t *ppos,
2176: 			   struct pipe_inode_info *pipe,
2177: 			   size_t len, unsigned int flags)
2179: 	struct tls_context *tls_ctx = tls_get_ctx(sock->sk);
2180: 	struct tls_sw_context_rx *ctx = tls_sw_ctx_rx(tls_ctx);
2182: 	struct sock *sk = sock->sk;
2189: 	err = tls_rx_reader_lock(sk, ctx, flags & SPLICE_F_NONBLOCK);
2190: 	if (err < 0)
2193: 	if (!skb_queue_empty(&ctx->rx_list)) {
2194: 		skb = __skb_dequeue(&ctx->rx_list);
2198: 		err = tls_rx_rec_wait(sk, NULL, flags & SPLICE_F_NONBLOCK,
2199: 				      true);
2200: 		if (err <= 0)
2203: 		memset(&darg.inargs, 0, sizeof(darg.inargs));
2205: 		err = tls_rx_one_record(sk, NULL, &darg);
2206: 		if (err < 0) {
2211: 		tls_rx_rec_done(ctx);
2212: 		skb = darg.skb;
2215: 	rxm = strp_msg(skb);
2216: 	tlm = tls_msg(skb);
2219: 	if (tlm->control != TLS_RECORD_TYPE_DATA) {
2225: 	copied = skb_splice_bits(skb, sk, rxm->offset, pipe, chunk, flags);
2246: int tls_sw_read_sock(struct sock *sk, read_descriptor_t *desc,
2249: 	struct tls_context *tls_ctx = tls_get_ctx(sk);
2250: 	struct tls_sw_context_rx *ctx = tls_sw_ctx_rx(tls_ctx);
2251: 	struct tls_prot_info *prot = &tls_ctx->prot_info;
2255: 	size_t flushed_at = 0;
2256: 	bool released = true;
2262: 	psock = sk_psock_get(sk);
2263: 	if (psock) {
2264: 		sk_psock_put(sk, psock);
2267: 	err = tls_rx_reader_acquire(sk, ctx, true);
2268: 	if (err < 0)
2273: 	if (err)
2276: 	decrypted = 0;
2278: 		if (!skb_queue_empty(&ctx->rx_list)) {
2279: 			skb = __skb_dequeue(&ctx->rx_list);
2280: 			rxm = strp_msg(skb);
2281: 			tlm = tls_msg(skb);
2285: 			err = tls_rx_rec_wait(sk, NULL, true, released);
2286: 			if (err <= 0)
2289: 			memset(&darg.inargs, 0, sizeof(darg.inargs));
2291: 			err = tls_rx_one_record(sk, NULL, &darg);
2292: 			if (err < 0) {
2293: 				tls_err_abort(sk, -EBADMSG);
2297: 			released = tls_read_flush_backlog(sk, prot, INT_MAX,
2298: 							  0, decrypted,
2299: 							  &flushed_at);
2300: 			skb = darg.skb;
2301: 			rxm = strp_msg(skb);
2302: 			tlm = tls_msg(skb);
2303: 			decrypted += rxm->full_len;
2305: 			tls_rx_rec_done(ctx);
2309: 		if (tlm->control != TLS_RECORD_TYPE_DATA) {
2314: 		used = read_actor(desc, skb, rxm->offset, rxm->full_len);
2315: 		if (used <= 0) {
2320: 		copied += used;
2321: 		if (used < rxm->full_len) {
2322: 			rxm->offset += used;
2323: 			rxm->full_len -= used;
2324: 			if (!desc->count)
2328: 			if (!desc->count)
2331: 	} while (skb);
2334: 	tls_rx_reader_release(sk, ctx);
2688: int tls_set_sw_offload(struct sock *sk, int tx)
2702: 	ctx = tls_get_ctx(sk);
2703: 	prot = &ctx->prot_info;
2705: 	if (tx) {
2706: 		ctx->priv_ctx_tx = init_ctx_tx(ctx, sk);
2707: 		if (!ctx->priv_ctx_tx)
2711: 		crypto_info = &ctx->crypto_send.info;
2715: 		ctx->priv_ctx_rx = init_ctx_rx(ctx);
2716: 		if (!ctx->priv_ctx_rx)
2720: 		crypto_info = &ctx->crypto_recv.info;
2725: 	cipher_desc = get_cipher_desc(crypto_info->cipher_type);
2726: 	if (!cipher_desc) {
2731: 	rc = init_prot_info(prot, crypto_info, cipher_desc);
2732: 	if (rc)
2735: 	iv = crypto_info_iv(crypto_info, cipher_desc);
2736: 	key = crypto_info_key(crypto_info, cipher_desc);
2737: 	salt = crypto_info_salt(crypto_info, cipher_desc);
2738: 	rec_seq = crypto_info_rec_seq(crypto_info, cipher_desc);
2744: 	if (!*aead) {
2745: 		*aead = crypto_alloc_aead(cipher_desc->cipher_name, 0, 0);
2746: 		if (IS_ERR(*aead)) {
2753: 	ctx->push_pending_record = tls_sw_push_pending_record;
2756: 	if (rc)
2760: 	if (rc)
2763: 	if (sw_ctx_rx) {
2771: 		rc = tls_strp_init(&sw_ctx_rx->strp, sk);
533: static int tls_do_encryption(struct sock *sk,
534: 			     struct tls_context *tls_ctx,
535: 			     struct tls_sw_context_tx *ctx,
536: 			     struct aead_request *aead_req,
537: 			     size_t data_len, u32 start)
539: 	struct tls_prot_info *prot = &tls_ctx->prot_info;
540: 	struct tls_rec *rec = ctx->open_rec;
541: 	struct sk_msg *msg_en = &rec->msg_encrypted;
546: 	switch (prot->cipher_type) {
548: 		rec->iv_data[0] = TLS_AES_CCM_IV_B0_BYTE;
552: 		rec->iv_data[0] = TLS_SM4_CCM_IV_B0_BYTE;
557: 	memcpy(&rec->iv_data[iv_offset], tls_ctx->tx.iv,
558: 	       prot->iv_size + prot->salt_size);
560: 	tls_xor_iv_with_seq(prot, rec->iv_data + iv_offset,
561: 			    tls_ctx->tx.rec_seq);
568: 	aead_request_set_tfm(aead_req, ctx->aead_send);
569: 	aead_request_set_ad(aead_req, prot->aad_size);
570: 	aead_request_set_crypt(aead_req, rec->sg_aead_in,
571: 			       rec->sg_aead_out,
572: 			       data_len, rec->iv_data);
574: 	aead_request_set_callback(aead_req, CRYPTO_TFM_REQ_MAY_BACKLOG,
575: 				  tls_encrypt_done, rec);
578: 	list_add_tail((struct list_head *)&rec->list, &ctx->tx_list);
579: 	DEBUG_NET_WARN_ON_ONCE(atomic_read(&ctx->encrypt_pending) < 1);
580: 	atomic_inc(&ctx->encrypt_pending);
582: 	rc = crypto_aead_encrypt(aead_req);
583: 	if (rc == -EBUSY) {
584: 		rc = tls_encrypt_async_wait(ctx);
585: 		rc = rc ?: -EINPROGRESS;
587: 	if (!rc || rc != -EINPROGRESS) {
588: 		atomic_dec(&ctx->encrypt_pending);
593: 	if (!rc) {
595: 	} else if (rc != -EINPROGRESS) {
601: 	ctx->open_rec = NULL;
602: 	tls_advance_record_sn(sk, prot, &tls_ctx->tx);
724: static int tls_push_record(struct sock *sk, int flags,
727: 	struct tls_context *tls_ctx = tls_get_ctx(sk);
728: 	struct tls_prot_info *prot = &tls_ctx->prot_info;
729: 	struct tls_sw_context_tx *ctx = tls_sw_ctx_tx(tls_ctx);
730: 	struct tls_rec *rec = ctx->open_rec, *tmp = NULL;
737: 	if (!rec)
740: 	msg_pl = &rec->msg_plaintext;
741: 	msg_en = &rec->msg_encrypted;
743: 	split_point = msg_pl->apply_bytes;
744: 	split = split_point && split_point < msg_pl->sg.size;
745: 	if (unlikely((!split &&
746: 		      msg_pl->sg.size +
747: 		      prot->overhead_size > msg_en->sg.size) ||
748: 		     (split &&
749: 		      split_point +
750: 		      prot->overhead_size > msg_en->sg.size))) {
752: 		split_point = msg_en->sg.size;
754: 	if (split) {
755: 		rc = tls_split_open_record(sk, rec, &tmp, msg_pl, msg_en,
756: 					   split_point, prot->overhead_size,
757: 					   &orig_end);
758: 		if (rc < 0)
765: 		if (!msg_pl->sg.size) {
766: 			tls_merge_open_record(sk, rec, tmp, orig_end);
768: 			msg_en = &rec->msg_encrypted;
769: 			split = false;
771: 		sk_msg_trim(sk, msg_en, msg_pl->sg.size +
772: 			    prot->overhead_size);
776: 	req = &rec->aead_req;
782: 	if (prot->version == TLS_1_3_VERSION) {
805: 	i = msg_en->sg.start;
808: 	tls_make_aad(rec->aad_space, msg_pl->sg.size + prot->tail_size,
809: 		     tls_ctx->tx.rec_seq, record_type, prot);
811: 	tls_fill_prepend(tls_ctx,
814: 			 msg_pl->sg.size + prot->tail_size,
817: 	tls_ctx->pending_open_record_frags = false;
819: 	rc = tls_do_encryption(sk, tls_ctx, ctx, req,
820: 			       msg_pl->sg.size + prot->tail_size, i);
821: 	if (rc < 0) {
822: 		if (rc != -EINPROGRESS) {
823: 			tls_err_abort(sk, -EBADMSG);
824: 			if (split) {
826: 				tls_merge_open_record(sk, rec, tmp, orig_end);
831: 	} else if (split) {
833: 		msg_en = &tmp->msg_encrypted;
834: 		sk_msg_trim(sk, msg_en, msg_pl->sg.size + prot->overhead_size);
839: 	return tls_tx_records(sk, flags);
842: static int bpf_exec_tx_verdict(struct sk_msg *msg, struct sock *sk,
843: 			       bool full_record, u8 record_type,
844: 			       ssize_t *copied, int flags)
846: 	struct tls_context *tls_ctx = tls_get_ctx(sk);
847: 	struct tls_sw_context_tx *ctx = tls_sw_ctx_tx(tls_ctx);
848: 	struct sk_msg msg_redir = { };
856: 	policy = !(flags & MSG_SENDPAGE_NOPOLICY);
857: 	psock = sk_psock_get(sk);
858: 	if (!psock || !policy) {
859: 		err = tls_push_record(sk, flags, record_type);
860: 		if (err && err != -EINPROGRESS && sk->sk_err == EBADMSG) {
861: 			*copied -= sk_msg_free(sk, msg);
862: 			tls_free_open_rec(sk);
863: 			err = -sk->sk_err;
865: 		if (psock)
866: 			sk_psock_put(sk, psock);
870: 	enospc = sk_msg_full(msg);
871: 	if (psock->eval == __SK_NONE) {
872: 		delta = msg->sg.size;
873: 		psock->eval = sk_psock_msg_verdict(sk, psock, msg);
874: 		delta -= msg->sg.size;
876: 	if (msg->cork_bytes && msg->cork_bytes > msg->sg.size &&
877: 	    !enospc && !full_record) {
881: 	msg->cork_bytes = 0;
882: 	send = msg->sg.size;
883: 	if (msg->apply_bytes && msg->apply_bytes < send)
884: 		send = msg->apply_bytes;
886: 	switch (psock->eval) {
888: 		err = tls_push_record(sk, flags, record_type);
889: 		if (err && err != -EINPROGRESS && sk->sk_err == EBADMSG) {
890: 			*copied -= sk_msg_free(sk, msg);
891: 			tls_free_open_rec(sk);
892: 			err = -sk->sk_err;
897: 		redir_ingress = psock->redir_ingress;
898: 		sk_redir = psock->sk_redir;
899: 		memcpy(&msg_redir, msg, sizeof(*msg));
900: 		if (msg->apply_bytes < send)
901: 			msg->apply_bytes = 0;
903: 			msg->apply_bytes -= send;
904: 		sk_msg_return_zero(sk, msg, send);
905: 		msg->sg.size -= send;
906: 		release_sock(sk);
907: 		err = tcp_bpf_sendmsg_redir(sk_redir, redir_ingress,
908: 					    &msg_redir, send, flags);
909: 		lock_sock(sk);
910: 		if (err < 0) {
911: 			*copied -= sk_msg_free_nocharge(sk, &msg_redir);
912: 			msg->sg.size = 0;
914: 		if (msg->sg.size == 0)
915: 			tls_free_open_rec(sk);
919: 		sk_msg_free_partial(sk, msg, send);
920: 		if (msg->apply_bytes < send)
921: 			msg->apply_bytes = 0;
923: 			msg->apply_bytes -= send;
924: 		if (msg->sg.size == 0)
925: 			tls_free_open_rec(sk);
927: 		err = -EACCES;
930: 	if (likely(!err)) {
931: 		bool reset_eval = !ctx->open_rec;
933: 		rec = ctx->open_rec;
934: 		if (rec) {
935: 			msg = &rec->msg_plaintext;
936: 			if (!msg->apply_bytes)
939: 		if (reset_eval) {
940: 			psock->eval = __SK_NONE;
941: 			if (psock->sk_redir) {
943: 				psock->sk_redir = NULL;
946: 		if (rec)
950: 	sk_psock_put(sk, psock);
954: static int tls_sw_push_pending_record(struct sock *sk, int flags)
956: 	struct tls_context *tls_ctx = tls_get_ctx(sk);
957: 	struct tls_sw_context_tx *ctx = tls_sw_ctx_tx(tls_ctx);
958: 	struct tls_rec *rec = ctx->open_rec;
962: 	if (!rec)
965: 	msg_pl = &rec->msg_plaintext;
966: 	copied = msg_pl->sg.size;
967: 	if (!copied)
970: 	return bpf_exec_tx_verdict(msg_pl, sk, true, TLS_RECORD_TYPE_DATA,
971: 				   &copied, flags);
file: /* tls_device.c */
1060: int tls_set_device_offload(struct sock *sk)
1073: 	ctx = tls_get_ctx(sk);
1074: 	prot = &ctx->prot_info;
1076: 	if (ctx->priv_ctx_tx)
1079: 	netdev = get_netdev_for_sock(sk);
1080: 	if (!netdev) {
1085: 	if (!(netdev->features & NETIF_F_HW_TLS_TX)) {
1090: 	crypto_info = &ctx->crypto_send.info;
1091: 	if (crypto_info->version != TLS_1_2_VERSION) {
1096: 	cipher_desc = get_cipher_desc(crypto_info->cipher_type);
1097: 	if (!cipher_desc || !cipher_desc->offloadable) {
1102: 	rc = init_prot_info(prot, crypto_info, cipher_desc);
1103: 	if (rc)
1106: 	iv = crypto_info_iv(crypto_info, cipher_desc);
1107: 	rec_seq = crypto_info_rec_seq(crypto_info, cipher_desc);
1110: 	memcpy(ctx->tx.rec_seq, rec_seq, cipher_desc->rec_seq);
1112: 	start_marker_record = kmalloc(sizeof(*start_marker_record), GFP_KERNEL);
1113: 	if (!start_marker_record) {
1118: 	offload_ctx = alloc_offload_ctx_tx(ctx);
1119: 	if (!offload_ctx) {
1124: 	rc = tls_sw_fallback_init(sk, offload_ctx, crypto_info);
1125: 	if (rc)
1128: 	start_marker_record->end_seq = tcp_sk(sk)->write_seq;
1133: 	clean_acked_data_enable(inet_csk(sk), &tls_icsk_clean_acked);
1134: 	ctx->push_pending_record = tls_device_push_pending_record;
1140: 	skb = tcp_write_queue_tail(sk);
1153: 	if (!(netdev->flags & IFF_UP)) {
1159: 	rc = netdev->tlsdev_ops->tls_dev_add(netdev, sk, TLS_OFFLOAD_CTX_DIR_TX,
1160: 					     &ctx->crypto_send.info,
1161: 					     tcp_sk(sk)->write_seq);
1162: 	trace_tls_device_offload_set(sk, TLS_OFFLOAD_CTX_DIR_TX,
1163: 				     tcp_sk(sk)->write_seq, rec_seq, rc);
1164: 	if (rc)
1167: 	tls_device_attach(ctx, sk, netdev);
1174: 	smp_store_release(&sk->sk_validate_xmit_skb, tls_validate_xmit_skb);
1181: 	clean_acked_data_disable(inet_csk(sk));
1193: int tls_set_device_offload_rx(struct sock *sk, struct tls_context *ctx)
1200: 	if (ctx->crypto_recv.info.version != TLS_1_2_VERSION)
1203: 	netdev = get_netdev_for_sock(sk);
1204: 	if (!netdev) {
1209: 	if (!(netdev->features & NETIF_F_HW_TLS_RX)) {
1223: 	if (!(netdev->flags & IFF_UP)) {
1228: 	context = kzalloc(sizeof(*context), GFP_KERNEL);
1229: 	if (!context) {
1236: 	rc = tls_set_sw_offload(sk, 0);
1237: 	if (rc)
1240: 	rc = netdev->tlsdev_ops->tls_dev_add(netdev, sk, TLS_OFFLOAD_CTX_DIR_RX,
1241: 					     &ctx->crypto_recv.info,
1242: 					     tcp_sk(sk)->copied_seq);
1243: 	info = (void *)&ctx->crypto_recv.info;
1244: 	trace_tls_device_offload_set(sk, TLS_OFFLOAD_CTX_DIR_RX,
1245: 				     tcp_sk(sk)->copied_seq, info->rec_seq, rc);
1246: 	if (rc)
1249: 	tls_device_attach(ctx, sk, netdev);
1258: 	tls_sw_free_resources_rx(sk);
421: static int tls_push_data(struct sock *sk,
422: 			 struct iov_iter *iter,
423: 			 size_t size, int flags,
424: 			 unsigned char record_type)
426: 	struct tls_context *tls_ctx = tls_get_ctx(sk);
427: 	struct tls_prot_info *prot = &tls_ctx->prot_info;
428: 	struct tls_offload_context_tx *ctx = tls_offload_ctx_tx(tls_ctx);
432: 	size_t orig_size = size;
434: 	bool more = false;
435: 	bool done = false;
439: 	if (flags &
440: 	    ~(MSG_MORE | MSG_DONTWAIT | MSG_NOSIGNAL |
441: 	      MSG_SPLICE_PAGES | MSG_EOR))
444: 	if ((flags & (MSG_MORE | MSG_EOR)) == (MSG_MORE | MSG_EOR))
447: 	if (unlikely(sk->sk_err))
448: 		return -sk->sk_err;
450: 	flags |= MSG_SENDPAGE_DECRYPTED;
451: 	tls_push_record_flags = flags | MSG_MORE;
453: 	timeo = sock_sndtimeo(sk, flags & MSG_DONTWAIT);
454: 	if (tls_is_partially_sent_record(tls_ctx)) {
455: 		rc = tls_push_partial_record(sk, tls_ctx, flags);
456: 		if (rc < 0)
460: 	pfrag = sk_page_frag(sk);
465: 	max_open_record_len = TLS_MAX_PAYLOAD_SIZE +
466: 			      prot->prepend_size;
468: 		rc = tls_do_allocation(sk, ctx, pfrag, prot->prepend_size);
469: 		if (unlikely(rc)) {
470: 			rc = sk_stream_wait_memory(sk, &timeo);
471: 			if (!rc)
474: 			record = ctx->open_record;
475: 			if (!record)
478: 			if (record_type != TLS_RECORD_TYPE_DATA) {
483: 				size = orig_size;
486: 			} else if (record->len > prot->prepend_size) {
493: 		record = ctx->open_record;
495: 		copy = min_t(size_t, size, max_open_record_len - record->len);
496: 		if (copy && (flags & MSG_SPLICE_PAGES)) {
498: 			struct page **pages = &zc_pfrag.page;
501: 			rc = iov_iter_extract_pages(iter, &pages,
502: 						    copy, 1, 0, &off);
503: 			if (rc <= 0) {
508: 			copy = rc;
510: 			if (WARN_ON_ONCE(!sendpage_ok(zc_pfrag.page))) {
511: 				iov_iter_revert(iter, copy);
516: 			zc_pfrag.offset = off;
517: 			zc_pfrag.size = copy;
518: 			tls_append_frag(record, &zc_pfrag, copy);
519: 		} else if (copy) {
520: 			copy = min_t(size_t, copy, pfrag->size - pfrag->offset);
522: 			rc = tls_device_copy_data(page_address(pfrag->page) +
523: 						  pfrag->offset, copy,
524: 						  iter);
525: 			if (rc)
527: 			tls_append_frag(record, pfrag, copy);
530: 		size -= copy;
531: 		if (!size) {
533: 			tls_push_record_flags = flags;
534: 			if (flags & MSG_MORE) {
539: 			done = true;
542: 		if (done || record->len >= max_open_record_len ||
543: 		    (record->num_frags >= MAX_SKB_FRAGS - 1)) {
544: 			tls_device_record_close(sk, tls_ctx, record,
545: 						pfrag, record_type);
547: 			rc = tls_push_record(sk,
548: 					     tls_ctx,
549: 					     ctx,
550: 					     record,
551: 					     tls_push_record_flags);
552: 			if (rc < 0)
555: 	} while (!done);
559: 	if (orig_size - size > 0)
560: 		rc = orig_size - size;
565: int tls_device_sendmsg(struct sock *sk, struct msghdr *msg, size_t size)
567: 	unsigned char record_type = TLS_RECORD_TYPE_DATA;
568: 	struct tls_context *tls_ctx = tls_get_ctx(sk);
571: 	if (!tls_ctx->zerocopy_sendfile)
572: 		msg->msg_flags &= ~MSG_SPLICE_PAGES;
575: 	lock_sock(sk);
577: 	if (unlikely(msg->msg_controllen)) {
578: 		rc = tls_process_cmsg(sk, msg, &record_type);
579: 		if (rc)
583: 	rc = tls_push_data(sk, &msg->msg_iter, size, msg->msg_flags,
584: 			   record_type);
587: 	release_sock(sk);
592: void tls_device_splice_eof(struct socket *sock)
594: 	struct sock *sk = sock->sk;
595: 	struct tls_context *tls_ctx = tls_get_ctx(sk);
596: 	struct iov_iter iter = {};
598: 	if (!tls_is_partially_sent_record(tls_ctx))
601: 	mutex_lock(&tls_ctx->tx_lock);
602: 	lock_sock(sk);
604: 	if (tls_is_partially_sent_record(tls_ctx)) {
605: 		iov_iter_bvec(&iter, ITER_SOURCE, NULL, 0, 0);
606: 		tls_push_data(sk, &iter, 0, 0, TLS_RECORD_TYPE_DATA);
673: static int tls_device_push_pending_record(struct sock *sk, int flags)
677: 	iov_iter_kvec(&iter, ITER_SOURCE, NULL, 0, 0);
678: 	return tls_push_data(sk, &iter, 0, flags, TLS_RECORD_TYPE_DATA);
file: /* tls_main.c */
612: static int do_tls_setsockopt_conf(struct sock *sk, sockptr_t optval,
613: 				  unsigned int optlen, int tx)
617: 	struct tls_context *ctx = tls_get_ctx(sk);
622: 	if (sockptr_is_null(optval) || (optlen < sizeof(*crypto_info)))
625: 	if (tx) {
626: 		crypto_info = &ctx->crypto_send.info;
629: 		crypto_info = &ctx->crypto_recv.info;
634: 	if (TLS_CRYPTO_INFO_READY(crypto_info))
637: 	rc = copy_from_sockptr(crypto_info, optval, sizeof(*crypto_info));
638: 	if (rc) {
643: 	rc = validate_crypto_info(crypto_info, alt_crypto_info);
644: 	if (rc)
647: 	cipher_desc = get_cipher_desc(crypto_info->cipher_type);
648: 	if (!cipher_desc) {
653: 	if (optlen != cipher_desc->crypto_info) {
658: 	rc = copy_from_sockptr_offset(crypto_info + 1, optval,
660: 				      optlen - sizeof(*crypto_info));
661: 	if (rc) {
666: 	if (tx) {
667: 		rc = tls_set_device_offload(sk);
669: 		if (!rc) {
670: 			TLS_INC_STATS(sock_net(sk), LINUX_MIB_TLSTXDEVICE);
671: 			TLS_INC_STATS(sock_net(sk), LINUX_MIB_TLSCURRTXDEVICE);
673: 			rc = tls_set_sw_offload(sk, 1);
674: 			if (rc)
676: 			TLS_INC_STATS(sock_net(sk), LINUX_MIB_TLSTXSW);
677: 			TLS_INC_STATS(sock_net(sk), LINUX_MIB_TLSCURRTXSW);
681: 		rc = tls_set_device_offload_rx(sk, ctx);
683: 		if (!rc) {
684: 			TLS_INC_STATS(sock_net(sk), LINUX_MIB_TLSRXDEVICE);
685: 			TLS_INC_STATS(sock_net(sk), LINUX_MIB_TLSCURRRXDEVICE);
687: 			rc = tls_set_sw_offload(sk, 0);
688: 			if (rc)
690: 			TLS_INC_STATS(sock_net(sk), LINUX_MIB_TLSRXSW);
691: 			TLS_INC_STATS(sock_net(sk), LINUX_MIB_TLSCURRRXSW);
694: 		tls_sw_strparser_arm(sk, ctx);
701: 	update_sk_prot(sk, ctx);
702: 	if (tx) {
703: 		ctx->sk_write_space = sk->sk_write_space;
704: 		sk->sk_write_space = tls_write_space;
769: static int do_tls_setsockopt(struct sock *sk, int optname, sockptr_t optval,
770: 			     unsigned int optlen)
774: 	switch (optname) {
777: 		lock_sock(sk);
778: 		rc = do_tls_setsockopt_conf(sk, optval, optlen,
779: 					    optname == TLS_TX);
780: 		release_sock(sk);
783: 		lock_sock(sk);
784: 		rc = do_tls_setsockopt_tx_zc(sk, optval, optlen);
785: 		release_sock(sk);
788: 		rc = do_tls_setsockopt_no_pad(sk, optval, optlen);
797: static int tls_setsockopt(struct sock *sk, int level, int optname,
798: 			  sockptr_t optval, unsigned int optlen)
800: 	struct tls_context *ctx = tls_get_ctx(sk);
802: 	if (level != SOL_TLS)
803: 		return ctx->sk_proto->setsockopt(sk, level, optname, optval,
804: 						 optlen);
806: 	return do_tls_setsockopt(sk, optname, optval, optlen);
859: static void tls_build_proto(struct sock *sk)
861: 	int ip_ver = sk->sk_family == AF_INET6 ? TLSV6 : TLSV4;
862: 	struct proto *prot = READ_ONCE(sk->sk_prot);
865: 	if (ip_ver == TLSV6 &&
866: 	    unlikely(prot != smp_load_acquire(&saved_tcpv6_prot))) {
868: 		if (likely(prot != saved_tcpv6_prot)) {
869: 			build_protos(tls_prots[TLSV6], prot);
871: 					sk->sk_socket->ops);
872: 			smp_store_release(&saved_tcpv6_prot, prot);
877: 	if (ip_ver == TLSV4 &&
878: 	    unlikely(prot != smp_load_acquire(&saved_tcpv4_prot))) {
880: 		if (likely(prot != saved_tcpv4_prot)) {
881: 			build_protos(tls_prots[TLSV4], prot);
883: 					sk->sk_socket->ops);
890: static void build_protos(struct proto prot[TLS_NUM_CONFIG][TLS_NUM_CONFIG],
894: 	prot[TLS_BASE][TLS_BASE].setsockopt	= tls_setsockopt;
934: static int tls_init(struct sock *sk)
939: 	tls_build_proto(sk);
952: 	if (sk->sk_state != TCP_ESTABLISHED)
956: 	write_lock_bh(&sk->sk_callback_lock);
957: 	ctx = tls_ctx_create(sk);
958: 	if (!ctx) {
965: 	update_sk_prot(sk, ctx);
967: 	write_unlock_bh(&sk->sk_callback_lock);
